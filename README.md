# DistributedCrawler

## 项目简介

基于 **Flink + Kafka/Redis + 爬虫框架 + 存储系统** 实现的简易分布式爬虫系统。该系统通过 **Flink 流式计算** 实现待爬取 URL 的去重，并将去重后的 URL 分发到 **Kafka** 或 **Redis** 消息队列中。随后，URL 被分发给各个爬虫节点进行数据爬取和解析，最终将处理后的数据存储到数据库中。系统设计遵循分布式思想，支持多进程并发爬取，能够高效地处理海量网页数据。

------

## 项目背景

分布式爬虫系统通过分布式架构，系统可以将爬取任务分发到多个节点并行执行，从而大幅提升爬取效率。

本项目旨在实现一个简易的分布式爬虫系统，结合 **Flink** 流式计算框架、**Kafka/Redis** 消息队列、**Scrapy** 爬虫框架以及 **MySQL** 数据库，构建一个高效、可扩展的分布式爬虫系统。

------

## 项目目标

本项目的核心目标是实现一个分布式爬虫系统，具备以下功能：

1. **URL 管理**：基于 Flink 实现 URL 的去重和管理。
2. **URL 分发**：通过 Kafka 或 Redis 将 URL 分发给多个爬虫节点。
3. **数据爬取**：使用 Scrapy 框架进行网页数据的并行爬取。
4. **数据解析**：对爬取的网页数据进行清洗、分词等处理。
5. **数据存储**：将解析后的数据存储到 MySQL 数据库中。
6. **数据查询**：提供 API 接口，支持对存储数据的查询。
7. **分布式调度**：基于 Redis 实现分布式调度，支持断点续爬。
8. **反爬虫策略**：支持随机 IP 代理、请求频率控制等反爬虫机制。

------

## 项目特点

1. **分布式架构**：基于 Flink、Kafka/Redis、Scrapy 等技术栈，实现高效的分布式爬取。
2. **URL 去重**：通过 Flink 流式计算框架实现 URL 的去重，避免重复爬取。
3. **多进程并发**：支持多进程并发爬取，提升系统吞吐量。
4. **反爬虫机制**：集成随机 IP 代理库，支持请求频率控制，有效应对反爬虫策略。
5. **数据存储与查询**：将爬取的数据存储到 MySQL 数据库中，并提供 Flask API 接口支持数据查询。
6. **可扩展性**：系统设计灵活，易于扩展新的爬虫节点或存储系统。

------

## 系统架构

以下是系统的整体架构图：

```python
+-------------------+       +-------------------+       +-------------------+
|   URL 管理模块    |       |   URL 分发模块    |       |   数据爬取模块    |
|  (Flink + Kafka)  | ----> |  (Kafka/Redis)    | ----> |  (Scrapy 爬虫)    |
+-------------------+       +-------------------+       +-------------------+
                                                                 |
                                                                 v
+-------------------+       +-------------------+       +-------------------+
|   数据解析模块    |       |   数据存储模块    |       |   数据查询模块    |
|  (数据清洗/分词)  | ----> |  (MySQL 数据库)   | ----> |  (Flask API)      |
+-------------------+       +-------------------+       +-------------------+
```

------

## 模块说明

### 1. URL 管理模块

- **功能**：基于 Flink 实现 URL 的去重和管理。
- **实现**：从 Kafka 中读取 URL，通过 Flink 进行去重处理，并将去重后的 URL 写回 Kafka 或 Redis。
- **核心代码**：`urlmanager.py`

### 2. URL 分发模块

- **功能**：将去重后的 URL 分发给多个爬虫节点。
- **实现**：支持 Kafka 和 Redis 两种消息队列，提供两种 URL 分发策略。
- **核心代码**：`kafka分发url.py`、`redis分发url.py`

### 3. 数据爬取模块

- **功能**：使用 Scrapy 框架进行网页数据的并行爬取。
- **实现**：支持多进程并发爬取，集成随机 IP 代理库，有效应对反爬虫策略。
- **核心代码**：`get_movie.py`

### 4. 数据解析模块

- **功能**：对爬取的网页数据进行清洗、分词等处理。
- **实现**：基于 Scrapy 的 Item Pipeline 实现数据解析和清洗。
- **核心代码**：`pipelines.py`

### 5. 数据存储模块

- **功能**：将解析后的数据存储到 MySQL 数据库中。
- **实现**：通过 Scrapy 的 Pipeline 将数据写入 MySQL 数据库。
- **核心代码**：`pipelines.py`

### 6. 数据查询模块

- **功能**：提供 API 接口，支持对存储数据的查询。
- **实现**：基于 Flask 框架实现数据查询 API。
- **核心代码**：`api.py`

### 7. 分布式调度模块

- **功能**：基于 Redis 实现分布式调度，支持断点续爬。
- **实现**：通过 Scrapy-Redis 实现分布式调度和任务分发。
- **核心代码**：`settings.py`

------

## 项目运行

### 环境要求

- Python 3.x
- Java 8+
- Flink 1.12+
- Kafka/Redis
- MySQL
- Scrapy
- Flask

### 运行步骤

1. **启动 Kafka/Redis**：
   - 启动 Kafka 服务，并创建 `step1` 和 `step2` 主题。
   - 启动 Redis 服务。
2. **运行 URL 管理模块**：
   - 编译并运行 `urlmanager.py`。
3. **运行 URL 分发模块**：
   - 运行 `kafka分发url.py` 或 `redis分发url.py`。
4. **运行数据爬取模块**：
   - 运行 `main.py`，启动 Scrapy 爬虫。
5. **运行数据查询模块**：
   - 运行 `api.py`，启动 Flask API 服务。

------

## 项目结构

```python
DistributedCrawler/
├── Distributed_Web_Crawler-main/
│   ├── URLManager.java            # URL 管理模块
│   ├── kafka分发url.py            # Kafka URL 分发模块
│   └── redis分发url.py            # Redis URL 分发模块
├── movie_test/
│   ├── spiders/
│   │   └── get_movie.py           # 数据爬取模块
│   ├── pipelines.py               # 数据存储模块
│   ├── api.py                     # 数据查询模块
│   ├── settings.py                # 配置文件
│   └── main.py                    # 多进程爬取入口
└── README.md                      # 项目文档
```

------

## 许可证

本项目采用 [MIT 许可证](https://license/)。

------

